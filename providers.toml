# cliProxyAPI 聚合网关配置 (物理实例 + 逻辑路由)

[global]
host = "0.0.0.0"
main_port = 8145                 # 主聚合端口
proxy = "${CLIPROXY_PROXY}"
request_retry = 3                # 实例内自动重试（403/5xx 等）
max_retry_interval = 30          # 最长等待冷却秒数
nonstream_keepalive_interval = 5 # 非流式保活（秒），减少长请求超时误判
streaming_keepalive_seconds = 15 # SSE 心跳（秒）
streaming_bootstrap_retries = 1  # 流式首包前安全重试
quota_switch_project = true      # 配额超限时自动切换账号/项目
quota_switch_preview_model = true # 配额超限时可自动切 preview 模型
request_log = true               # 详细请求/响应日志（用于对话分析）
logs_max_total_size_mb = 102400  # 日志目录上限 100GB，超限自动删最旧文件
lb_auth_cooldown_ms = 1800000    # LB 认证类错误冷却 30 分钟
lb_validation_cooldown_ms = 43200000 # LB 验证类 403 冷却 12 小时
lb_quota_cooldown_ms = 43200000  # LB 配额类错误冷却 12 小时
lb_max_target_retries = 1        # LB 首包前最多自动切换重试次数
lb_auto_upgrade_enabled = false

[global.lb_model_router]
enabled = true
shadow_only = false
activation_models = ["auto", "auto-canary"]
default_model = "M2.5"
log_factors = true

# === 语义类别路由 (优先于阈值规则) ===
[[global.lb_model_router.categories]]
name = "architecture"
priority = 500
target_model = "opus4.6"
signals = ["task_category:architecture", "system_prompt_type:plan_mode", "keyword:architect|design|system.*design|scalability"]

[[global.lb_model_router.categories]]
name = "code-review"
priority = 400
target_model = "gpt5.3"
signals = ["task_category:code-review", "system_prompt_type:review", "keyword:review|audit|refactor|rewrite", "keyword:debug|排查|根因"]

[[global.lb_model_router.categories]]
name = "visual-coding"
priority = 350
target_model = "g3p"
signals = ["task_category:visual-coding", "keyword:frontend|前端|UI|CSS|tailwind|视觉|responsive|animation"]

[[global.lb_model_router.categories]]
name = "coding"
priority = 300
target_model = "sonnet4.6"
signals = ["task_category:coding", "has_code_context:true", "tool_profile:coding"]

[[global.lb_model_router.categories]]
name = "explore"
priority = 200
target_model = "M2.5"
signals = ["task_category:explore", "keyword:find|search|where|explain|what|how", "tool_profile:read", "tool_profile:explore"]

[[global.lb_model_router.categories]]
name = "quick"
priority = 100
target_model = "M2.5"
signals = ["task_category:quick", "keyword:^(hi|hello|thanks|ok|hey)$", "messages_count:<=1"]

# === 阈值规则 (categories 未命中时的 fallback) ===

# ── T5: M2.5 — 故障恢复（跨 provider fallback）──
# 注意：failure_reason 不可用于路由，failure_streak 只是累计计数。
# failure_recovery 只在 categories 全部未命中时触发（rules 被 !hitRule 守卫）。
# 未命中 category 的请求正常走 default_model=M2.5，故障时回退到同一目标即可。
# M2.5 (official/MiniMax) 与 zenmux 完全跨 provider，无 thinking signature 问题。
[[global.lb_model_router.rules]]
name = "failure_recovery"
priority = 500
target_model = "M2.5"
match = "any"
[[global.lb_model_router.rules.when]]
field = "failure_streak"
op = ">="
value = 2

# ── T4: opus4.6 — 超长对话或大量工具 ──
[[global.lb_model_router.rules]]
name = "high_complexity"
priority = 400
target_model = "opus4.6"
match = "any"
[[global.lb_model_router.rules.when]]
field = "messages_count"
op = ">="
value = 50
[[global.lb_model_router.rules.when]]
field = "tools_count"
op = ">="
value = 10

# ── T3: sonnet4.6 — 中高复杂度 ──
[[global.lb_model_router.rules]]
name = "medium_high"
priority = 300
target_model = "sonnet4.6"
match = "any"
[[global.lb_model_router.rules.when]]
field = "messages_count"
op = ">="
value = 25
[[global.lb_model_router.rules.when]]
field = "tools_count"
op = ">="
value = 6
[[global.lb_model_router.rules.when]]
field = "prompt_chars"
op = ">="
value = 15000

# ── T2: M2.5 — 中等复杂度 ──
[[global.lb_model_router.rules]]
name = "medium"
priority = 200
target_model = "M2.5"
match = "any"
[[global.lb_model_router.rules.when]]
field = "messages_count"
op = ">="
value = 10
[[global.lb_model_router.rules.when]]
field = "tools_count"
op = ">="
value = 3
[[global.lb_model_router.rules.when]]
field = "prompt_chars"
op = ">="
value = 5000

# ── T1: M2.5 — 默认兜底（靠 default_model）──

# ==========================================
# 1. 物理实例定义 (Instances)
#    每个 [instances.name] 对应一个独立进程
# ==========================================

# --- 实例 A: Official (官方原生 Auth 聚合) ---
# 包含 Gemini (Google) 和 OpenAI (Codex/ChatGPT) + MiniMax
# 均使用 OAuth 登录，无需在此填 Key
[instances.official]
port = 8146
request_retry = 1
max_retry_interval = 5
providers = [
  # 1. Gemini (Google OAuth)
  # 需运行: ./cliproxy --login
  { type = "gemini", rotation_strategy = "round-robin" },

  # 2. OpenAI Official (Codex/ChatGPT OAuth)
  # 需运行: ./cliproxy --codex-login
  { type = "codex", rotation_strategy = "round-robin" },

  # 3. MiniMax (API Key)
  { type = "minimax", base_url = "https://api.minimaxi.com/anthropic", api_keys = [
    "${MINIMAX_KEY}",
  ] },
]

# --- 实例 B: Zenmux (聚合渠道 - API Key) ---
[instances.zenmux]
port = 8147
disable_cooling = true
request_retry = 2
max_retry_interval = 15
providers = [
  # Endpoint 1: OpenAI 兼容 (GPT, DeepSeek, Kimi)
  { type = "openai", base_url = "https://zenmux.ai/api/v1", api_keys = [
    "${ZENMUX_KEY}",
  ] },
  # Endpoint 2: Anthropic 兼容 (Claude)
  { type = "anthropic", base_url = "https://zenmux.ai/api/anthropic", api_keys = [
    "${ZENMUX_KEY}",
  ] },
  # Endpoint 3: Vertex AI 兼容 (Gemini)
  { type = "gemini", base_url = "https://zenmux.ai/api/vertex-ai", api_keys = [
    "${ZENMUX_KEY}",
  ] },
]

# ==========================================
# 2. 聚合路由与映射 (Routing)
#    定义主网关对外暴露的模型，以及分发策略
#    格式: "对外ID" = [ { instance="实例名", provider="逻辑Provider类型", model="内部映射名", weight=权重 }, ... ]
# ==========================================

[routing]

"opus4.6" = [
  { instance = "zenmux", provider = "anthropic", model = "anthropic/claude-opus-4.6", weight = 100, params = { "max_tokens_max" = 24000 } },
]

"sonnet4.6" = [
  { instance = "zenmux", provider = "anthropic", model = "anthropic/claude-sonnet-4.6", weight = 100, params = { "max_tokens_max" = 16000 } },
]

"M2.5" = [
  { instance = "official", provider = "minimax", model = "MiniMax-M2.5", weight = 100, params = { "max_tokens_max" = 204800 } },
]

"g3f" = [
  { instance = "official", provider = "gemini", model = "gemini-3-flash-preview", weight = 999, params = { "max_tokens_max" = 64000 } },
]

"g3p" = [
  { instance = "official", provider = "gemini", model = "gemini-3-pro-preview", weight = 999, params = { "thinking_level" = "high", "max_tokens_max" = 64000 } },
]

"g3i" = [
  { instance = "official", provider = "gemini", model = "gemini-3-pro-image-preview", weight = 999 },
]

"gpt5.3" = [
  { instance = "official", provider = "codex", model = "gpt-5.3-codex", weight = 1, params = { "reasoning_effort" = "high" } },
]

"gpt5.2" = [
  { instance = "official", provider = "codex", model = "gpt-5.2", weight = 80, params = { "reasoning_effort" = "high" } },
  { instance = "zenmux", provider = "openai", model = "openai/gpt-5.2", weight = 20, params = { "reasoning_effort" = "high" } },
]
